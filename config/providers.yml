# Provider Configuration
#
# This file defines all available LLM providers. Each provider needs:
# - type: Either 'anthropic' (uses Anthropic SDK) or 'openai-compatible' (uses OpenAI SDK)
# - api_key_env: Environment variable name for the API key
# - base_url: API endpoint URL (only for openai-compatible providers)
#
# To add a new provider:
# 1. Add an entry here with the provider configuration
# 2. Create a models file at config/models/{provider_name}.yml

default:
  provider: anthropic
  model: claude-sonnet-4-5-20250929

providers:
  anthropic:
    type: anthropic
    api_key_env: ANTHROPIC_API_KEY

  openai:
    type: openai-compatible
    base_url: https://api.openai.com/v1
    api_key_env: OPENAI_API_KEY

  gemini:
    type: openai-compatible
    base_url: https://generativelanguage.googleapis.com/v1beta/openai/
    api_key_env: GOOGLE_API_KEY

  perplexity:
    type: openai-compatible
    base_url: https://api.perplexity.ai
    api_key_env: PERPLEXITY_API_KEY

  groq:
    type: openai-compatible
    base_url: https://api.groq.com/openai/v1
    api_key_env: GROQ_API_KEY

  deepseek:
    type: openai-compatible
    base_url: https://api.deepseek.com
    api_key_env: DEEPSEEK_API_KEY

  mistral:
    type: openai-compatible
    base_url: https://api.mistral.ai/v1
    api_key_env: MISTRAL_API_KEY

  openrouter:
    type: openai-compatible
    base_url: https://openrouter.ai/api/v1
    api_key_env: OPENROUTER_API_KEY

  xai:
    type: openai-compatible
    base_url: https://api.x.ai/v1
    api_key_env: XAI_API_KEY

  # -------------------------------------------------------------------------
  # Local Ollama Instances
  # -------------------------------------------------------------------------
  # Ollama providers discover models dynamically from the Ollama API.
  # No API key is required. You can configure multiple instances by giving
  # each a unique name and specifying its base_url.
  #
  # Example configurations:
  #
  #   ollama-local:
  #     type: ollama
  #     base_url: http://localhost:11434
  #
  #   ollama-server:
  #     type: ollama
  #     base_url: http://192.168.1.100:11434
  #
  #   ollama-gpu:
  #     type: ollama
  #     base_url: http://gpu-server.local:11434
  #
  # Models from each instance will be available as:
  #   {provider-name}-{model-name}
  # For example: ollama-server-llama3.2:latest

  # Uncomment and configure your Ollama instances below:
  # ollama:
  #   type: ollama
  #   base_url: http://localhost:11434
