# Multi-Provider LLM Proxy
# Standalone Docker Compose configuration
#
# Ports:
#   - 11434: Ollama/OpenAI compatible API (no auth required)
#   - 8080: Admin UI (password protected)
#
# Quick Start:
#   1. Copy .env.example to .env
#   2. Add your API keys to .env
#   3. Run: docker compose up -d
#
# For Docker Swarm deployments, use docker-compose.swarm.yml instead.

services:
  llm-proxy:
    image: ghcr.io/benhumphry/ollama-llm-proxy:latest
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "11434:11434" # API server (Ollama/OpenAI compatible)
      - "8080:8080" # Admin UI
    volumes:
      - llm-proxy-data:/data # Persist database
    env_file:
      - .env
    environment:
      # Server configuration (defaults)
      - PORT=11434
      - HOST=0.0.0.0
      - ADMIN_PORT=8080
      - ADMIN_HOST=0.0.0.0
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:11434/')",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

volumes:
  llm-proxy-data:
